stages:
  - build
  - docker-push
  - deploy

variables:
  DOCKER_IMAGE_NAME: ${DOCKER_IMAGE_NAME}
  DOCKER_USERNAME: ${DOCKER_USERNAME}
  DOCKER_PASSWORD: ${DOCKER_PASSWORD}

# Build stage - runs on main branch
build:
  stage: build
  image: eclipse-temurin:21-jdk
  script:
    - cd accommodation-be
    - chmod +x ./gradlew
    - ./gradlew clean build -x test
    - cp $(find build/libs -name "*.jar" | head -n 1) app.jar
  artifacts:
    paths:
      - accommodation-be/app.jar
      - accommodation-be/Dockerfile
      - accommodation-be/k8s/deployment.yaml
      - accommodation-be/k8s/service.yaml
      - accommodation-be/k8s/ingress.yaml
    expire_in: 1 hour
  only:
    - main

# Docker push - builds and pushes image with latest tag
docker-push:
  stage: docker-push
  image:
    name: gcr.io/kaniko-project/executor:v1.23.2-debug
    entrypoint: [""]
  dependencies:
    - build
  script:
    - mkdir -p /kaniko/.docker
    - echo "{\"auths\":{\"https://index.docker.io/v1/\":{\"auth\":\"$(printf "%s:%s" "${DOCKER_USERNAME}" "${DOCKER_PASSWORD}" | base64 | tr -d '\n')\"}}}" > /kaniko/.docker/config.json
    - /kaniko/executor
      --context "${CI_PROJECT_DIR}/accommodation-be"
      --dockerfile "${CI_PROJECT_DIR}/accommodation-be/Dockerfile"
      --destination "${DOCKER_IMAGE_NAME}:latest"
      --destination "${DOCKER_IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}"
  only:
    - main

# Deploy to Production - auto deploy from main branch
deploy:
  stage: deploy
  image: alpine:latest
  dependencies:
    - build
  before_script:
    - apk add --no-cache openssh bash
    - mkdir -p ~/.ssh
    - printf "%s\n" "$EC2_SSH_KEY" > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - |
      if ssh-keyscan -H -T 10 $EC2_HOST >> ~/.ssh/known_hosts 2>/dev/null; then
        echo "SSH host key added successfully"
      else
        echo "StrictHostKeyChecking no" >> ~/.ssh/config
        chmod 644 ~/.ssh/config
        echo "Using StrictHostKeyChecking no as fallback"
      fi
  script:
    # Generating Kubernetes ConfigMap manifest for production
    - |
      printf "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: accommodation-ti-config\ndata:\n  DATABASE_URL_PROD: \"%s\"\n  DATABASE_USERNAME: \"%s\"\n" \
        "$DATABASE_URL_PROD" "$DATABASE_USERNAME" > config.yaml

    # Generating Kubernetes Secret manifest for production
    - |
      printf "apiVersion: v1\nkind: Secret\nmetadata:\n  name: accommodation-ti-secret\ntype: Opaque\nstringData:\n  DATABASE_PASSWORD: \"%s\"\n  JWT_SECRET_KEY: \"%s\"\n  CORS_ALLOWED_ORIGINS: \"%s\"\n" \
        "$DATABASE_PASSWORD" "$JWT_SECRET_KEY" "$CORS_ALLOWED_ORIGINS" > secret.yaml

    # Update deployment.yaml with production image tag
    - sed "s|\${DOCKER_IMAGE_NAME}:\${IMAGE_TAG}|$DOCKER_IMAGE_NAME:latest|" accommodation-be/k8s/deployment.yaml > deployment.yaml

    # Transfer configuration files to EC2
    - scp config.yaml $EC2_USER@$EC2_HOST:~/app/k8s/accommodation-ti-config.yaml
    - scp secret.yaml $EC2_USER@$EC2_HOST:~/app/k8s/accommodation-ti-secret.yaml
    - scp deployment.yaml $EC2_USER@$EC2_HOST:~/app/k8s/accommodation-ti-deployment.yaml
    - scp accommodation-be/k8s/service.yaml $EC2_USER@$EC2_HOST:~/app/k8s/accommodation-ti-service.yaml
    - scp accommodation-be/k8s/ingress.yaml $EC2_USER@$EC2_HOST:~/app/k8s/accommodation-ti-ingress.yaml

    # Apply Kubernetes manifests on EC2
    - ssh $EC2_USER@$EC2_HOST "sudo k3s kubectl apply -f ~/app/k8s/accommodation-ti-config.yaml"
    - ssh $EC2_USER@$EC2_HOST "sudo k3s kubectl apply -f ~/app/k8s/accommodation-ti-secret.yaml"
    - ssh $EC2_USER@$EC2_HOST "sudo k3s kubectl apply -f ~/app/k8s/accommodation-ti-deployment.yaml"
    - ssh $EC2_USER@$EC2_HOST "sudo k3s kubectl apply -f ~/app/k8s/accommodation-ti-service.yaml"
    - ssh $EC2_USER@$EC2_HOST "sudo k3s kubectl apply -f ~/app/k8s/accommodation-ti-ingress.yaml"
  environment:
    name: production
    url: https://2306214510-be.hafizmuh.site
  only:
    - main
